大家好，我是自动化2003班的张硕，我们组队员有我、张欣宇、姜滔，我们小组的课题为：机器人下棋——海克斯棋


我的汇报将从以下四个部分讲起


首先是下棋机器人概况


我们通过编程工具python与汉语言软件，混合编程完成了一款基本功能成熟的海克斯下棋软件，可以实现基本的人人对战，人机对战，
同时，也介绍了海克斯棋的规则，策略、算法


其次是Alpha Zero-Hex算法的介绍，小组中我完成的部分是项目的整体规划以及这一部分


首先介绍下基本游戏规则，海克斯棋为红蓝双方，红色先手，交互下棋，连通对应颜色边界即可获得胜利


我们的思路是智能化改造Alpha Zero算法
下面介绍Alpha Zero算法状态空间的定义，
Alpha Zero采用的是强化学习的思路，强化学习相当重要的一点是状态空间的定义，
这里我们采用的是最经典的11*11的棋盘格式，
海克斯的棋盘共计11*11为121格
状态空间由11*11*（2n+1）的张量表示，Alpha Zero的状态定义包括红蓝双方各自前n步的格局，除此之外，还需加上一个矩阵表示当前状态由哪一方下棋，
由此状态定义，我们可以通过卷积网络初步训练出价值网络与策略网络


再然后是蒙特卡洛树搜索，Alpha Zero下棋的真正决策是由mcts决定的，做决定前，算法会在脑海里模拟成千上万局，用以预知每一种动作的效果，
每一次模拟的过程就是建立mcts树的过程，可分为以下四步


在选择阶段，状态节点选出目前可行的情况，根据已有的mcts选择的当前状态的动作，此时采用min-max思路，假设对方也采用mcts产出的策略，具体每一步的分数计算公式如下，选出下一状态的最佳动作，依次重复即可获得胜利，
最终采用价值与奖励的均值，回溯为Q值的更新，这就是一个循环的完整过程


在最佳动作选择中，是依靠上述的公式来计算得到的，
Q表示上述模拟得到的动作价值，
Na表示访问次数，
Π为策略网络，
η是待调参的超参数
在Alpha Zero中，策略网络、价值网络与mct是同时更新的，
在动作a选择次数少时，主要由策略网络起作用，鼓励探索次数更少的选择，
在动作a选择次数多时，主要由动作价值起作用，鼓励探索胜率高的，


而后是效果的展示，可以看出，算法可以完美运行，并且训练出的机器棋力相当不错




第三部分是汉语言板块的实现，这一部分是张欣宇同学完成的


在汉语言部分，我们小组实现了海克斯棋在汉语言上的人人对战，人机对战模式，
我们的棋盘棋子均为图片元件显示；为了便于棋盘不同棋子颜色的存贮，我们分别定义了相应的集合存贮红蓝空三种棋子；
为实现鼠标点击下棋实现，我们定义当前下棋玩家，并当作全局变量储存，循环检测有效实现了玩家的轮换


其次是游戏中的胜利检测，算法首先检测棋盘是否有可用位置，
而后从边缘起始位置，依次遍历临近棋子，标记访问过的位置，
若检测到了另一边缘的终止位置，即判断胜利，
算法采用深度有限搜索，在遍历时，优先探索深度最深的棋子，
在每一步落子完成后均会进行胜利判断，
完成了该步的汉语言算法，已经成功独立实现了人人对战海克斯棋小游戏了

最终，我们将汉语言与python程序混合编程，成功完成了在汉语言软件上的人人对战，人机对战功能，效果出乎意料的不错




第四部分是教学测展示，这一部分是由姜滔同学制作完成，


首先我们制作了教学测的封面，在教、学、测分别设了三处按钮，分别能跳转三部分的程序，右侧是教学测的内容概述


在教的部分，我们充分介绍了hex棋的起源与发展、介绍了海克斯棋的基本规则、讲述了海克斯棋的算法介绍、最后教授了海克斯棋的进阶玩法以及常见策略


这里展示了以上四部分内容，从背景介绍、规则介绍、策略介绍、再到算法介绍、完整教授了海克斯棋方方面面的知识


然后是学的部分，我们完整演示了我们搭建的汉语言海克斯棋小程序的使用方法以及用户体验，带领用户领略小程序的各个部分


最后是测的部分，我们制作了简单的程序编写的题目，设计了上述教授内容对应的简单小测试，最后进入游戏，可以体验人机对战，人人对战模式的海克斯棋游戏

我的汇报到此结束，请老师批评指正


